digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139657556675456 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	139657547233600 [label=AddmmBackward0]
	139657547231920 -> 139657547233600
	139657590138176 [label="fc.bias
 (1000)" fillcolor=lightblue]
	139657590138176 -> 139657547231920
	139657547231920 [label=AccumulateGrad]
	139657547234512 -> 139657547233600
	139657547234512 [label=ReshapeAliasBackward0]
	139657547234464 -> 139657547234512
	139657547234464 [label=MeanBackward1]
	139657547234752 -> 139657547234464
	139657547234752 [label=ReluBackward0]
	139657547234224 -> 139657547234752
	139657547234224 [label=AddBackward0]
	139657490676752 -> 139657547234224
	139657490676752 [label=NativeBatchNormBackward0]
	139657490676416 -> 139657490676752
	139657490676416 [label=ConvolutionBackward0]
	139657490676800 -> 139657490676416
	139657490676800 [label=ReluBackward0]
	139657490675840 -> 139657490676800
	139657490675840 [label=NativeBatchNormBackward0]
	139657490676512 -> 139657490675840
	139657490676512 [label=ConvolutionBackward0]
	139657490675216 -> 139657490676512
	139657490675216 [label=ReluBackward0]
	139657490676608 -> 139657490675216
	139657490676608 [label=AddBackward0]
	139657490677616 -> 139657490676608
	139657490677616 [label=NativeBatchNormBackward0]
	139657490676560 -> 139657490677616
	139657490676560 [label=ConvolutionBackward0]
	139657490809232 -> 139657490676560
	139657490809232 [label=ReluBackward0]
	139657490809472 -> 139657490809232
	139657490809472 [label=NativeBatchNormBackward0]
	139657490810720 -> 139657490809472
	139657490810720 [label=ConvolutionBackward0]
	139657490809664 -> 139657490810720
	139657490809664 [label=ReluBackward0]
	139657490809760 -> 139657490809664
	139657490809760 [label=AddBackward0]
	139657490811920 -> 139657490809760
	139657490811920 [label=NativeBatchNormBackward0]
	139657490809520 -> 139657490811920
	139657490809520 [label=ConvolutionBackward0]
	139657490811440 -> 139657490809520
	139657490811440 [label=ReluBackward0]
	139657490812208 -> 139657490811440
	139657490812208 [label=NativeBatchNormBackward0]
	139657490811200 -> 139657490812208
	139657490811200 [label=ConvolutionBackward0]
	139657490808896 -> 139657490811200
	139657490808896 [label=ReluBackward0]
	139657490811296 -> 139657490808896
	139657490811296 [label=AddBackward0]
	139657490812496 -> 139657490811296
	139657490812496 [label=NativeBatchNormBackward0]
	139657547428000 -> 139657490812496
	139657547428000 [label=ConvolutionBackward0]
	139657547428192 -> 139657547428000
	139657547428192 [label=ReluBackward0]
	139657547428336 -> 139657547428192
	139657547428336 [label=NativeBatchNormBackward0]
	139657547428432 -> 139657547428336
	139657547428432 [label=ConvolutionBackward0]
	139657547428624 -> 139657547428432
	139657547428624 [label=ReluBackward0]
	139657547428768 -> 139657547428624
	139657547428768 [label=AddBackward0]
	139657547428864 -> 139657547428768
	139657547428864 [label=NativeBatchNormBackward0]
	139657547429008 -> 139657547428864
	139657547429008 [label=ConvolutionBackward0]
	139657547429200 -> 139657547429008
	139657547429200 [label=ReluBackward0]
	139657547429344 -> 139657547429200
	139657547429344 [label=NativeBatchNormBackward0]
	139657547429440 -> 139657547429344
	139657547429440 [label=ConvolutionBackward0]
	139657547428816 -> 139657547429440
	139657547428816 [label=ReluBackward0]
	139657547429728 -> 139657547428816
	139657547429728 [label=AddBackward0]
	139657547429824 -> 139657547429728
	139657547429824 [label=NativeBatchNormBackward0]
	139657547429968 -> 139657547429824
	139657547429968 [label=ConvolutionBackward0]
	139657547430160 -> 139657547429968
	139657547430160 [label=ReluBackward0]
	139657547430304 -> 139657547430160
	139657547430304 [label=NativeBatchNormBackward0]
	139657547430400 -> 139657547430304
	139657547430400 [label=ConvolutionBackward0]
	139657547430592 -> 139657547430400
	139657547430592 [label=ReluBackward0]
	139657547430736 -> 139657547430592
	139657547430736 [label=AddBackward0]
	139657547430832 -> 139657547430736
	139657547430832 [label=NativeBatchNormBackward0]
	139657547430976 -> 139657547430832
	139657547430976 [label=ConvolutionBackward0]
	139657547431168 -> 139657547430976
	139657547431168 [label=ReluBackward0]
	139657547431312 -> 139657547431168
	139657547431312 [label=NativeBatchNormBackward0]
	139657547431408 -> 139657547431312
	139657547431408 [label=ConvolutionBackward0]
	139657547430784 -> 139657547431408
	139657547430784 [label=ReluBackward0]
	139657547431696 -> 139657547430784
	139657547431696 [label=AddBackward0]
	139657547431792 -> 139657547431696
	139657547431792 [label=NativeBatchNormBackward0]
	139657547431888 -> 139657547431792
	139657547431888 [label=ConvolutionBackward0]
	139657547419904 -> 139657547431888
	139657547419904 [label=ReluBackward0]
	139657547420048 -> 139657547419904
	139657547420048 [label=NativeBatchNormBackward0]
	139657547420144 -> 139657547420048
	139657547420144 [label=ConvolutionBackward0]
	139657547431744 -> 139657547420144
	139657547431744 [label=MaxPool2DWithIndicesBackward0]
	139657547420432 -> 139657547431744
	139657547420432 [label=ReluBackward0]
	139657547420528 -> 139657547420432
	139657547420528 [label=NativeBatchNormBackward0]
	139657547420624 -> 139657547420528
	139657547420624 [label=ConvolutionBackward0]
	139657547420816 -> 139657547420624
	139657590425616 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139657590425616 -> 139657547420816
	139657547420816 [label=AccumulateGrad]
	139657547420576 -> 139657547420528
	139657590425696 [label="bn1.weight
 (64)" fillcolor=lightblue]
	139657590425696 -> 139657547420576
	139657547420576 [label=AccumulateGrad]
	139657547420240 -> 139657547420528
	139657590425776 [label="bn1.bias
 (64)" fillcolor=lightblue]
	139657590425776 -> 139657547420240
	139657547420240 [label=AccumulateGrad]
	139657547420336 -> 139657547420144
	139657590426416 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139657590426416 -> 139657547420336
	139657547420336 [label=AccumulateGrad]
	139657547420096 -> 139657547420048
	139657590426336 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139657590426336 -> 139657547420096
	139657547420096 [label=AccumulateGrad]
	139657547419952 -> 139657547420048
	139657590426496 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139657590426496 -> 139657547419952
	139657547419952 [label=AccumulateGrad]
	139657547419856 -> 139657547431888
	139657590426976 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139657590426976 -> 139657547419856
	139657547419856 [label=AccumulateGrad]
	139657547431840 -> 139657547431792
	139657590426896 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139657590426896 -> 139657547431840
	139657547431840 [label=AccumulateGrad]
	139657547419712 -> 139657547431792
	139657590427056 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139657590427056 -> 139657547419712
	139657547419712 [label=AccumulateGrad]
	139657547431744 -> 139657547431696
	139657547431600 -> 139657547431408
	139657590427456 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139657590427456 -> 139657547431600
	139657547431600 [label=AccumulateGrad]
	139657547431360 -> 139657547431312
	139657590427376 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139657590427376 -> 139657547431360
	139657547431360 [label=AccumulateGrad]
	139657547431216 -> 139657547431312
	139657590427536 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139657590427536 -> 139657547431216
	139657547431216 [label=AccumulateGrad]
	139657547431120 -> 139657547430976
	139657589961168 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139657589961168 -> 139657547431120
	139657547431120 [label=AccumulateGrad]
	139657547430928 -> 139657547430832
	139657589961088 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139657589961088 -> 139657547430928
	139657547430928 [label=AccumulateGrad]
	139657547430880 -> 139657547430832
	139657589961248 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139657589961248 -> 139657547430880
	139657547430880 [label=AccumulateGrad]
	139657547430784 -> 139657547430736
	139657547430544 -> 139657547430400
	139657589962368 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	139657589962368 -> 139657547430544
	139657547430544 [label=AccumulateGrad]
	139657547430352 -> 139657547430304
	139657589962288 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139657589962288 -> 139657547430352
	139657547430352 [label=AccumulateGrad]
	139657547430208 -> 139657547430304
	139657589962448 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139657589962448 -> 139657547430208
	139657547430208 [label=AccumulateGrad]
	139657547430112 -> 139657547429968
	139657589962928 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139657589962928 -> 139657547430112
	139657547430112 [label=AccumulateGrad]
	139657547429920 -> 139657547429824
	139657589962848 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139657589962848 -> 139657547429920
	139657547429920 [label=AccumulateGrad]
	139657547429872 -> 139657547429824
	139657589963008 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139657589963008 -> 139657547429872
	139657547429872 [label=AccumulateGrad]
	139657547429776 -> 139657547429728
	139657547429776 [label=NativeBatchNormBackward0]
	139657547430496 -> 139657547429776
	139657547430496 [label=ConvolutionBackward0]
	139657547430592 -> 139657547430496
	139657547430640 -> 139657547430496
	139657589961648 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	139657589961648 -> 139657547430640
	139657547430640 [label=AccumulateGrad]
	139657547430064 -> 139657547429776
	139657589961728 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	139657589961728 -> 139657547430064
	139657547430064 [label=AccumulateGrad]
	139657547430016 -> 139657547429776
	139657589961808 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	139657589961808 -> 139657547430016
	139657547430016 [label=AccumulateGrad]
	139657547429632 -> 139657547429440
	139657589963408 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139657589963408 -> 139657547429632
	139657547429632 [label=AccumulateGrad]
	139657547429392 -> 139657547429344
	139657589963328 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139657589963328 -> 139657547429392
	139657547429392 [label=AccumulateGrad]
	139657547429248 -> 139657547429344
	139657589963488 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139657589963488 -> 139657547429248
	139657547429248 [label=AccumulateGrad]
	139657547429152 -> 139657547429008
	139657589963968 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139657589963968 -> 139657547429152
	139657547429152 [label=AccumulateGrad]
	139657547428960 -> 139657547428864
	139657589963888 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139657589963888 -> 139657547428960
	139657547428960 [label=AccumulateGrad]
	139657547428912 -> 139657547428864
	139657589964048 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139657589964048 -> 139657547428912
	139657547428912 [label=AccumulateGrad]
	139657547428816 -> 139657547428768
	139657547428576 -> 139657547428432
	139657590047184 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	139657590047184 -> 139657547428576
	139657547428576 [label=AccumulateGrad]
	139657547428384 -> 139657547428336
	139657590047104 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139657590047104 -> 139657547428384
	139657547428384 [label=AccumulateGrad]
	139657547428240 -> 139657547428336
	139657590047264 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139657590047264 -> 139657547428240
	139657547428240 [label=AccumulateGrad]
	139657547428144 -> 139657547428000
	139657590047744 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139657590047744 -> 139657547428144
	139657547428144 [label=AccumulateGrad]
	139657547427952 -> 139657490812496
	139657590047664 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139657590047664 -> 139657547427952
	139657547427952 [label=AccumulateGrad]
	139657547427904 -> 139657490812496
	139657590047824 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139657590047824 -> 139657547427904
	139657547427904 [label=AccumulateGrad]
	139657490811776 -> 139657490811296
	139657490811776 [label=NativeBatchNormBackward0]
	139657547428528 -> 139657490811776
	139657547428528 [label=ConvolutionBackward0]
	139657547428624 -> 139657547428528
	139657547428672 -> 139657547428528
	139657589964448 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	139657589964448 -> 139657547428672
	139657547428672 [label=AccumulateGrad]
	139657547428096 -> 139657490811776
	139657589964528 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139657589964528 -> 139657547428096
	139657547428096 [label=AccumulateGrad]
	139657547428048 -> 139657490811776
	139657589964608 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139657589964608 -> 139657547428048
	139657547428048 [label=AccumulateGrad]
	139657490810816 -> 139657490811200
	139657590048224 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139657590048224 -> 139657490810816
	139657490810816 [label=AccumulateGrad]
	139657490812112 -> 139657490812208
	139657590048144 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139657590048144 -> 139657490812112
	139657490812112 [label=AccumulateGrad]
	139657490810624 -> 139657490812208
	139657590048304 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139657590048304 -> 139657490810624
	139657490810624 [label=AccumulateGrad]
	139657490809952 -> 139657490809520
	139657590048784 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139657590048784 -> 139657490809952
	139657490809952 [label=AccumulateGrad]
	139657490809856 -> 139657490811920
	139657590048704 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139657590048704 -> 139657490809856
	139657490809856 [label=AccumulateGrad]
	139657490812064 -> 139657490811920
	139657590048864 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139657590048864 -> 139657490812064
	139657490812064 [label=AccumulateGrad]
	139657490808896 -> 139657490809760
	139657490810336 -> 139657490810720
	139657590049984 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	139657590049984 -> 139657490810336
	139657490810336 [label=AccumulateGrad]
	139657490810528 -> 139657490809472
	139657590049904 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139657590049904 -> 139657490810528
	139657490810528 [label=AccumulateGrad]
	139657490811152 -> 139657490809472
	139657590050064 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139657590050064 -> 139657490811152
	139657490811152 [label=AccumulateGrad]
	139657490810432 -> 139657490676560
	139657590050544 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139657590050544 -> 139657490810432
	139657490810432 [label=AccumulateGrad]
	139657490676224 -> 139657490677616
	139657590050464 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139657590050464 -> 139657490676224
	139657490676224 [label=AccumulateGrad]
	139657490809808 -> 139657490677616
	139657590050624 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139657590050624 -> 139657490809808
	139657490809808 [label=AccumulateGrad]
	139657490676176 -> 139657490676608
	139657490676176 [label=NativeBatchNormBackward0]
	139657490810000 -> 139657490676176
	139657490810000 [label=ConvolutionBackward0]
	139657490809664 -> 139657490810000
	139657490809136 -> 139657490810000
	139657590049264 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139657590049264 -> 139657490809136
	139657490809136 [label=AccumulateGrad]
	139657490812016 -> 139657490676176
	139657590049344 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139657590049344 -> 139657490812016
	139657490812016 [label=AccumulateGrad]
	139657490809424 -> 139657490676176
	139657590049424 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139657590049424 -> 139657490809424
	139657490809424 [label=AccumulateGrad]
	139657490675504 -> 139657490676512
	139657590137136 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139657590137136 -> 139657490675504
	139657490675504 [label=AccumulateGrad]
	139657490675984 -> 139657490675840
	139657590137056 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139657590137056 -> 139657490675984
	139657490675984 [label=AccumulateGrad]
	139657490675408 -> 139657490675840
	139657590137216 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139657590137216 -> 139657490675408
	139657490675408 [label=AccumulateGrad]
	139657490675696 -> 139657490676416
	139657590137696 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139657590137696 -> 139657490675696
	139657490675696 [label=AccumulateGrad]
	139657490676320 -> 139657490676752
	139657590137616 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139657590137616 -> 139657490676320
	139657490676320 [label=AccumulateGrad]
	139657490675936 -> 139657490676752
	139657590137776 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139657590137776 -> 139657490675936
	139657490675936 [label=AccumulateGrad]
	139657490675216 -> 139657547234224
	139657547231536 -> 139657547233600
	139657547231536 [label=TBackward0]
	139657547233792 -> 139657547231536
	139657590138096 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	139657590138096 -> 139657547233792
	139657547233792 [label=AccumulateGrad]
	139657547233600 -> 139657556675456
}
